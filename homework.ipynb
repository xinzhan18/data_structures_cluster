{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock market clustering\n",
    "\n",
    "_Data Structures and Algorithms_\n",
    "\n",
    "_Imperial College Business School_\n",
    "\n",
    "\n",
    "---\n",
    "This assignment is divided into three parts. In the first part, you will work on `pandas` data analysis. In the second part, you will implement a clustering algorithm to group companies based on their stock price movements. In the final part, you will explore ways to extend and improve this analysis. \n",
    "\n",
    "---\n",
    "\n",
    "**The assignment is graded not only on correctness but also on the presentation of the results.** Try to make the results of your calculations easy to read with eg string formatting, do some plots if you find them useful, and comment your code.\n",
    "\n",
    "**There are no OK tests to test your functions in this assignment.** It is intended to set you up working on a real problem where you have to explore data and the problem to figure out your approach. The first part will also require you to use a search engine to find the right pandas functions to use to analyse your data. Some potentially useful pandas functions are listed in the file `veryUseful.py`. \n",
    "\n",
    "**You're working as a group, so you may wish to divide the work into smaller pieces.** Some of you may want to start working on the Pandas part, and others on the algorithm part. There is a set of intermediary results available for testing your algorithm, so you can start immediately on both parts. See the details below in question 3.\n",
    "\n",
    "**You may use generative AI (such as chatGPT) in the last part 3 of the assignment, but not in the other parts.** If you use generative AI (or other sources), please explain clearly how you have used it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your group\n",
    "\n",
    "You'll complete this assignment in your assigned study groups. If you are unsure about your group, please contact the programme team.\n",
    "\n",
    "## Submission\n",
    "\n",
    "Create a zip file containing your submission notebook (and possible other files required to run your code). Submit by uploading the zip file on the Hub: Go to DSA -> Assessment -> Homework 3. Only one submission is needed for your group. **Please note that the Hub has a file size limit.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pandas\n",
    "\n",
    "**30% of grade**\n",
    "\n",
    "In the previous homework, we used lists to study stock prices. The `pandas` library provides some more effective tools for data analysis.\n",
    "\n",
    "The assignment comes with two files containing company data:\n",
    "- `SP_500_firms.csv` with firm and ticker names\n",
    "- `SP_500_close_2015.csv` with stock price data for 2015\n",
    "\n",
    "Let's first load up this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load data into Python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings (\"ignore\")\n",
    "\n",
    "def read_names_into_dict():\n",
    "    \"\"\"\n",
    "    Read company names into a dictionary\n",
    "    \"\"\"\n",
    "    d = dict()\n",
    "    with open(\"SP_500_firms.csv\") as csvfile:\n",
    "        input_file = csv.DictReader(csvfile)\n",
    "        for row in input_file:\n",
    "            #print(row)\n",
    "            d[row['Symbol']] = [row['Name'],row['Sector']]\n",
    "    return d\n",
    "\n",
    "names_dict = read_names_into_dict()\n",
    "comp_names = names_dict.keys()\n",
    "\n",
    "# Read price data with pandas\n",
    "filename = 'SP_500_close_2015.csv'\n",
    "price_data = pd.read_csv(filename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Returns\n",
    "\n",
    "In the previous homework, we calculated stock price _returns_ over a period of time. The return is defined as the percentage change, so the return between periods $t-1$ and $t$ for stock price $p$ would be\n",
    "\n",
    "$$\n",
    "x_t = \\frac{p_t - p_{t-1}}{p_{t-1}}.\n",
    "$$\n",
    "\n",
    "Calculate the returns in `pandas` for all the stocks in `price_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate company returns in this cell\n",
    "price_data.sort_index(inplace=True)\n",
    "prices_return =pd.DataFrame()\n",
    "for idx,col in enumerate(price_data.columns):\n",
    "    prices_return[col] = price_data[col] / price_data[col].shift(1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data['MMM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1: Highest and lowest daily returns\n",
    "\n",
    "Use pandas to find the 10 highest daily returns amongst all companies. Search online for what were the reasons behind the highest returns. Present your results in a clean and immediately readable form.\n",
    "\n",
    "Repeat with the lowest daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_return = prices_return.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_data = pd.melt(prices_return,id_vars='Date',value_vars=prices_return.columns.tolist()[1:],value_name='return',var_name='stock_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest Daily Return\n",
    "print('Top 10 Highest Daily Return')\n",
    "melt_data.sort_values('return',ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowest Daily Return\n",
    "print('Top 10 Lowest Daily Return')\n",
    "melt_data.sort_values('return',ascending=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Highest and lowest yearly returns\n",
    "\n",
    "Find the 10 highest yearly returns amongst all companies. Present your results in a clean and immediately readable form.\n",
    "\n",
    "Repeat with the lowest yearly returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_return = pd.DataFrame(price_data.iloc[-1] / price_data.iloc[0] - 1)\n",
    "yearly_return.columns = ['yearly_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest Yearly Return\n",
    "print('Top 10 Yearly Daily Return')\n",
    "yearly_return.sort_values('yearly_return',ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowest Yearly Return\n",
    "print('Top 10 Lowest Daily Return')\n",
    "yearly_return.sort_values('yearly_return',ascending=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: Highest and lowest volatilities\n",
    "\n",
    "Find the 10 highest yearly volatilities (standard deviations) amongst all companies. Present your results in a clean and immediately readable form.\n",
    "\n",
    "Repeat with the lowest volatilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_return = prices_return.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt_data_std.columns[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "melt_data = melt_data.dropna()\n",
    "melt_data_std = melt_data.groupby(['stock_name','year'])['return'].std().reset_index()\n",
    "melt_data_std.columns = ['stock_name','year','std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 highest yearly volatilities\n",
    "print('Top 10 highest yearly volatilities')\n",
    "melt_data_std.sort_values('std',ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 lowest yearly volatilities\n",
    "print('Top 10 lowest yearly volatilities')\n",
    "melt_data_std.sort_values('std',ascending=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Correlations\n",
    "\n",
    "Analysts often care about the _correlation_ of stock prices between firms. Correlation measures the statistical similarity between the two prices' movements. If the prices move very similarly, the correlation of their _returns_  is close to 1. If they tend to make exactly the opposite movements (ie one price moves up and the other one down), the correlation is close to -1. If there is no clear statistical relationship between the movements of two stock prices, the correlation in their returns is close to zero.\n",
    "\n",
    "For a sample of stock price returns $x,y$ with observations for $n$ days, the correlation $r_{xy}$ between $x$ and $y$ can be calculated as:\n",
    "\n",
    "$$\n",
    "r_{xy} = \\frac{\\sum x_i y_i - n \\bar{x}\\bar{y}}{ns_x s_y} = {\\frac {n\\sum x_{i}y_{i}-\\sum x_{i}\\sum y_{i}}{{\\sqrt {n\\sum x_{i}^{2}-(\\sum x_{i})^{2}}}~{\\sqrt {n\\sum y_{i}^{2}-(\\sum y_{i})^{2}}}}}.\n",
    "$$\n",
    "\n",
    "Here $\\bar{x}$ refers to the average value of $x$ over the $n$ observations, and $s_x$ to its standard deviation.\n",
    "\n",
    "Based on time series of the stock returns we just computed, we can calculate a  correlation value for each pair of stocks, for example between MSFT (Microsoft) and AAPL (Apple). This gives us a measure of the similarity between the two stocks in this time period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate all correlations between companies. You can search online for a `pandas` or `numpy` function that does this directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "prices_return.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "\n",
    "Next, analyse the correlations between the companies:\n",
    "- Define functions to print out the $n$ top and bottom correlated companies for any given company. \n",
    "- Use your functions to study the following companies in the tech sector: Amazon, Microsoft, Facebook, Apple, and Google. Comment on the results. Which (possibly other) companies are they most closely related to in terms of highest correlations? Would you have expected the results you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def find_top_corr(target, prices_return, top_n=10, bottom_n = 10):\n",
    "    '''\n",
    "    target: target company name\n",
    "    prices_return: stock return matrix\n",
    "    top_n: the number of top related stock\n",
    "    bottom_n: the number of bottom related stock\n",
    "    '''\n",
    "    \n",
    "    corr_matrix = prices_return.corr()\n",
    "    try:\n",
    "        \n",
    "        bottom_corr = corr_matrix[target].sort_values(ascending=True)[:bottom_n]\n",
    "        top_corr = corr_matrix[target].sort_values(ascending=False)[1:top_n+1]\n",
    "        \n",
    "        bottom_corr_df = pd.DataFrame(bottom_corr).reset_index()\n",
    "        bottom_corr_df.columns = ['stock_code','corr']\n",
    "        bottom_corr_df['stock_name'] = bottom_corr_df['stock_code'].apply(lambda x: names_dict[x][0])\n",
    "        \n",
    "        top_corr_df = pd.DataFrame(top_corr).reset_index()\n",
    "        top_corr_df.columns = ['stock_code','corr']\n",
    "        top_corr_df['stock_name'] = top_corr_df['stock_code'].apply(lambda x: names_dict[x][0])\n",
    "        \n",
    "        print(f'Top {top_n} corr company of {target}')\n",
    "        print(top_corr_df)\n",
    "        print('-'*50)\n",
    "        print(f'Bottom {bottom_n} corr company of {target}')\n",
    "        print(bottom_corr_df)\n",
    "        print('-'*50)\n",
    "        print(f'{names_dict[top_corr.index[0]][0]}({top_corr.index[0]}) are the most closely related to {names_dict[target][0]} ({target})')\n",
    "    except:\n",
    "        print('please inter a correct stock code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top_corr('MSFT', prices_return, top_n=10, bottom_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top_corr('AMZN', prices_return, top_n=10, bottom_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top_corr('FB', prices_return, top_n=10, bottom_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top_corr('AAPL', prices_return, top_n=10, bottom_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top_corr('GOOG', prices_return, top_n=10, bottom_n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Clustering\n",
    "\n",
    "**30% of grade**\n",
    "\n",
    "In this part of the assignment, you will develop a clustering algorithm to study the similarity of different stocks. \n",
    "\n",
    "The general purpose of clustering analysis is dividing a set of objects into groups that are somehow \"similar\" to each other. It is a widespread tool used for exploratory data analysis in diverse fields in both science and business. For example, in marketing analytics, cluster analysis is employed to group consumers into segments based on their characteristics or _features_, such as age, post code, purchase history, etc. These features are somehow aggregated to compare the similarity between consumers. Based on this similarity, a clustering algorithm then divides the consumers into segments.\n",
    "\n",
    "We will apply this idea on stock market data to identify groups of stocks that perform similarly over time. There are many reasons for grouping stocks together, such as analysing trading strategies, risk management, or simply presenting stock market information. Publicly traded companies are often grouped together by simple features such as the industry they operate in (eg tech companies or pharma companies), but here we'll take a data-driven approach, grouping together stocks that perform similarly over time. \n",
    "\n",
    "Cluster analysis is an umbrella term for many different algorithmic approaches. Here you'll develop one that's based on the concept of `greedy` algorithm design, specified below. You'll also have the opportunity to explore other approaches using Python libraries.\n",
    "\n",
    "What is a good measure for stocks \"performing similarly\" to use for clustering. Let's use the one we just calculated: correlations in their returns. How can we use this similarity information for clustering? We now have access to all correlations between stock returns in S&P 500. We can think of this as a _graph_ as follows. The _nodes_ of the graph are the stocks (eg MSFT and AAPL). The _edges_ between them are the correlations, which we have just calculated between each stock, where the value of the correlation is the edge weight. Notice that since we have the correlations between all companies, this is a _dense_ graph, where all possible edges exist.\n",
    "\n",
    "We thus have a graph representing pairwise \"similarity\" scores in correlations, and we want to divide the graph into clusters. There are many possible ways to do this, but here we'll use a _greedy_ algorithm design. The algorithm is as follows:\n",
    "\n",
    "1. Sort the edges in the graph by their weight (ie the correlation), pick a number $k$ for the number of iterations of the algorithm\n",
    "2. Create single-node sets from each node in the graph\n",
    "3. Repeat $k$ times:\n",
    "\t1. Pick the graph edge with the highest correlation\n",
    "\t2. Combine the two sets containing the source and the destination of the edge\n",
    "\t3. Repeat with the next-highest weight edge\n",
    "4. Return the remaining sets after the $k$ iterations \n",
    "\n",
    "What does the algorithm do? It first initializes a graph with no connections, where each node is in a separate set. Then in the main loop, it runs through the $k$ highest-weighted edges, and adds connections at those edges. This leads to sets being combined (or \"merged\"). The result is \"groups\" of stocks determined by the highest correlations between the stock returns. These are your stock clusters.\n",
    "\n",
    "For example, suppose that the toy graph below represents four stocks: A,B,C,D and their return correlations. Suppose we pick $k=2$ and run the algorithm. \n",
    "\n",
    "<img src=\"cluster0.png\" alt=\"cluster0\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "The algorithm would begin by initializing four separate sets of one node each: {A}, {B}, {C}, {D}. It would then first connect C and D because of their correlation 0.95, resulting in just three sets: {A}, {B}, and {C,D}. Then it would connect A and B, resulting in two sets of two nodes each: {A,B}, and {C,D}. These would be our clusters for $k=2$.\n",
    "\n",
    "### Question 3: Implementing the algorithm\n",
    "\n",
    "Your task is to implement the clustering algorithm using the functions below. First, for convenience in implementing the algorithm, let's create a list of the correlations from the pandas data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_list(correl):\n",
    "    \"\"\"\n",
    "    Creates a list of correlations from a pandas dataframe of correlations\n",
    "    \n",
    "    Parameters:\n",
    "        correl: pandas dataframe of correlations\n",
    "    \n",
    "    Returns:\n",
    "        list of correlations containing tuples of form (correlation, ticker1, ticker2)\n",
    "    \"\"\"\n",
    "    n_comp = len(correl.columns)\n",
    "    comp_names = list(correl.columns)\n",
    "    # Faster if we use a numpy matrix\n",
    "    correl_mat = correl.to_numpy()\n",
    "    L = [] # create list\n",
    "    for i in range(n_comp):\n",
    "        for j in range(i+1,n_comp):\n",
    "            L.append((correl_mat[i,j],comp_names[i],comp_names[j]))\n",
    "    return L\n",
    "\n",
    "correl = prices_return.corr()\n",
    "edges = create_correlation_list(correl)\n",
    "edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's turn to the algorithm itself. Consider the example above, repeated here.\n",
    "\n",
    "<img src=\"cluster0.png\" alt=\"cluster0\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "Suppose we pick $k=3$ and have sorted the edge list in step 1 of the algorithm. How should we represent the clusters in step 2? One great way is to use a dictionary where each _key_ is a node, and each _value_ is another node that this node \"points to\". A cluster is then a chain of these links, which we represent as a dictionary.\n",
    "\n",
    "In step 2 of the algorithm, we start with four nodes that point to themselves, ie the dictionary `{'A':'A','B':'B','C':'C','D':'D'}`. When a node points to itself, it ends the chain. Here the clusters are thus just the nodes themselves, as in the figure below.\n",
    "\n",
    "<img src=\"cluster1.png\" alt=\"cluster1\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "Let's walk through the algorithm's next steps. We first look at the highest-weight edge, which is between C and D. These clusters will be combined. In terms of the dictionary, this means that one of them will not point to itself, but to the other one (here it does not matter which one). So we make the dictionary at `C` point to `D`. The dictionary becomes `{'A':'A','B':'B','C':'D','D':'D'}`.\n",
    "\n",
    "<img src=\"cluster2.png\" alt=\"cluster2\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "The next highest correlation is between A and B, so these clusters would be combined. The dictionary becomes `{'A':'B','B':'B','C':'D','D':'D'}`.\n",
    "\n",
    "<img src=\"cluster3.png\" alt=\"cluster3\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "The third highest correlation is between C and B. Let's think about combining these clusters using the dictionary we have. Looking up `B`, we get `B`: the node B is in the _bottom_ of the chain representing its cluster. But when we look up `C`, it points to `D`. Should we make `C` point to `B`? No - that would leave nothing  pointing at `D`, and `C` and `D` should remain connected! We could perhaps have `C` somehow point at both nodes, but that could become complicated, so we'll do the following instead. We'll follow the chain to the bottom. In the dictionary, we look up `C` and see that it points to `D`. We then look up `D` which points to itself, so `D` is the _bottom_ node. We then pick one of the bottom nodes `B` and `D`, and make it point to the other. We then have the dictionary `{'A':'B','B':'B','C':'D','D':'B'}`, and the corresponding clustering in the figure below.\n",
    "\n",
    "<img src=\"cluster4.png\" alt=\"cluster4\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "In other words, we'll keep track of clusters in a dictionary such that **each cluster has exactly one bottom node**. To do this, we need a mechanism for following a cluster to the bottom. You'll implement this in the function `find_bottom` below. The function takes as input a node and a dictionary, and runs through the \"chain\" in the dictionary until it finds a bottom node pointing to itself.\n",
    "\n",
    "The other thing we'll need to do is combine clusters by connecting two nodes. This means taking the two nodes, finding the bottom node for each node's cluster, and making one point to the other. You'll implement this in the function `merge_clusters` below.\n",
    "\n",
    "Finally, you'll need to set up the algorithm by sorting the correlations, and then looping through this merging $k$ times. You'll implement this in the function `cluster_correlations` below. This completes the algorithm.\n",
    "\n",
    "But there is one more thing. If you only keep track of a dictionary like `{'A':'B','B':'B','C':'D','D':'B'}`, how do you actually find the clusters from the dictionary? A convenient way is to store some extra information: the \"starting nodes\" of each cluster to which no other node links. For example, above these \"starting nodes\" would include all nodes `A,B,C,D` in the beginning, but only `A` and `C` in the end. If we keep track of these, we can then write a function that starts from each such remaining \"starting node\", works through to the bottom, and creates the cluster along the way. You'll implement this in the function `construct_sets` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediary results\n",
    "\n",
    "You can load a pre-computed set of results up to this point using the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MMM', 'ABT', 'ABBV', 'ACN', 'ATVI', 'AYI', 'ADBE', 'AAP', 'AES', 'AET']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.598666164029738, 'MMM', 'ABT'),\n",
       " (0.32263699601940254, 'MMM', 'ABBV'),\n",
       " (0.6320593488560189, 'MMM', 'ACN'),\n",
       " (0.41855006701119907, 'MMM', 'ATVI'),\n",
       " (0.4508974957132859, 'MMM', 'AYI'),\n",
       " (0.4687548443045165, 'MMM', 'ADBE'),\n",
       " (0.25713165217544326, 'MMM', 'AAP'),\n",
       " (0.33537796741224424, 'MMM', 'AES'),\n",
       " (0.31737374099675925, 'MMM', 'AET'),\n",
       " (0.5059306055816828, 'MMM', 'AMG')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load intermediary results from a \"pickle\" file\n",
    "# You can use these with your algorithm below\n",
    "import pickle\n",
    "file_name = 'cluster_correlations'\n",
    "with open(file_name, \"rb\") as f:\n",
    "    correl = pickle.load(f)\n",
    "    edges = pickle.load(f)\n",
    "\n",
    "firms = list(correl.columns)\n",
    "print(firms[:10])\n",
    "edges[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering implementation\n",
    "\n",
    "Complete the following functions to implement the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F': {'B', 'E', 'F'}, 'D': {'A', 'D', 'C'}}\n"
     ]
    }
   ],
   "source": [
    "d = {'A':'C','B':'E','C':'D','D':'D','E':'F','F':'F'}\n",
    "\n",
    "\n",
    "find_bottom('A', d)\n",
    "starters = {'A', 'B','D'}\n",
    "\n",
    "print(construct_sets(starters, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.95, 'C', 'D'), (0.8, 'A', 'B'), (0.68, 'B', 'C'), (0.63, 'A', 'C'), (0.32, 'B', 'D'), (0.05, 'A', 'D')]\n",
      "{'A', 'C'}\n",
      "{'B': 'D', 'C': 'D', 'D': 'D', 'A': 'B'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'D': {'A', 'B', 'C', 'D'}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = [(0.8, 'A', 'B'),\n",
    "        (0.63, 'A', 'C'),\n",
    "        (0.05, 'A', 'D'),\n",
    "        (0.68, 'B', 'C'),\n",
    "        (0.32, 'B', 'D'),\n",
    "        (0.95, 'C', 'D')]\n",
    "starters = {'A','B','C','D'}\n",
    "set_starters, next_nodes = cluster_correlations(edges, starters, k=3)\n",
    "print(set_starters)\n",
    "print(next_nodes)\n",
    "# starters = {'A', 'C'}\n",
    "# next_nodes = {'A':'B','B':'B','C':'D','D':'B'}\n",
    "result = construct_sets(set_starters, next_nodes)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_bottom(node, next_nodes):\n",
    "    \"\"\"\n",
    "    Find the \"bottom\" of a cluster starting from node in dictionary next_nodes\n",
    "\n",
    "    Parameters:\n",
    "        node: starting node\n",
    "        next_nodes: dictionary of node connections\n",
    "\n",
    "    Returns:\n",
    "        the bottom node in the cluster\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    while node != next_nodes[node]:\n",
    "        node = next_nodes[node]\n",
    "    return node\n",
    "\n",
    "\n",
    "def merge_sets(node1, node2, next_nodes, set_starters):\n",
    "    \"\"\"\n",
    "    Merges the clusters containing node1, node2 using the connections dictionary next_nodes.\n",
    "    Also removes any bottom node which is no longer a \"starting node\" from set_starters.\n",
    "\n",
    "    Parameters:\n",
    "        node1: first node the set of which will be merged\n",
    "        node2: second node the set of which will be merged\n",
    "        next_nodes: dictionary of node connections\n",
    "        set_starters: set of nodes that \"start\" a cluster\n",
    "\n",
    "    Returns:\n",
    "        does this function need to return something?\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    bottom1 = find_bottom(node1, next_nodes)\n",
    "    bottom2 = find_bottom(node2, next_nodes)\n",
    "    next_nodes[bottom1] = bottom2\n",
    "    if bottom2 in set_starters:\n",
    "        set_starters.remove(bottom2)\n",
    "\n",
    "\n",
    "def cluster_correlations(edge_list, firms, k=200):\n",
    "    \"\"\"\n",
    "    A mystery clustering algorithm\n",
    "     \n",
    "    Parameters:\n",
    "         edge_list - list of edges of the form (weight,source,destination)\n",
    "         firms - list of firms (tickers)\n",
    "         k - number of iterations of algorithm\n",
    "\n",
    "    Returns:\n",
    "         next_nodes - dictionary to store clusters as \"pointers\"\n",
    "            - the dictionary keys are the nodes and the values are the node in the same cluster that the key node points to\n",
    "         set_starters - set of nodes that no other node points to (this will be used to construct the sets below)\n",
    "\n",
    "    Algorithm:\n",
    "         1 sort edges by weight (highest correlation first)\n",
    "         2 initialize next_nodes so that each node points to itself (single-node clusters)\n",
    "         3 take highest correlation edge\n",
    "            check if the source and destination are in the same cluster using find_bottom\n",
    "            if not, merge the source and destination nodes' clusters using merge_sets\n",
    "         4 if max iterations not reached, repeat 3 with next highest correlation\n",
    "         (meanwhile also keep track of the \"set starters\" ie nodes that have nothing pointing to them for later use)\n",
    "    \"\"\"\n",
    "    # Sort edges\n",
    "    sorted_edges = sorted(edge_list, key=lambda x: x[0], reverse=True)\n",
    "    print(sorted_edges)\n",
    "    # Initialize dictionary of pointers\n",
    "    next_nodes = {node: node for node in firms}\n",
    "    # Keep track of \"starting nodes\", ie nodes that no other node points to in next_nodes\n",
    "    set_starters = {node for node in firms}\n",
    "\n",
    "    # Loop k times\n",
    "    for i in range(k):\n",
    "        # Your algorithm here\n",
    "        for edge in sorted_edges:\n",
    "            if find_bottom(edge[1], next_nodes) != find_bottom(edge[2], next_nodes):\n",
    "                merge_sets(edge[1], edge[2], next_nodes, set_starters)\n",
    "                break\n",
    "                \n",
    "        \n",
    "    return set_starters, next_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've run the algorithm, we'll need to construct the clusters. You can use the function below to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sets(set_starters, next_nodes):\n",
    "    \"\"\"\n",
    "    Constructs sets (clusters) from the next_nodes dictionary\n",
    "    \n",
    "    Parameters:\n",
    "        set_starters: set of starting nodes \n",
    "        next_nodes: dictionary of connections\n",
    "    \n",
    "    Returns: \n",
    "        dictionary of sets (clusters):\n",
    "            key - bottom node of set; value - set of all nodes in the cluster\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialise an empty dictionary \n",
    "    all_sets = dict()\n",
    "    \n",
    "    # Loop:\n",
    "    # Start from each set starter node\n",
    "    # Construct a \"current set\" with all nodes on the way to bottom node\n",
    "    # If bottom node is already a key of all_sets, combine the \"current set\" with the one in all_sets,\n",
    "    # Otherwise add \"current set\" to all_sets\n",
    "    for s in set_starters:\n",
    "        cur_set = set()\n",
    "        cur_set.add(s)\n",
    "        p = s\n",
    "        while next_nodes[p] != p:\n",
    "            p = next_nodes[p]\n",
    "            cur_set.add(p)\n",
    "            \n",
    "        if p not in all_sets:\n",
    "            all_sets[p] = cur_set\n",
    "        else: \n",
    "            for item in cur_set:\n",
    "                all_sets[p].add(item)\n",
    "    return all_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_starters, next_nodes = cluster_correlations(edges, firms, k=200)\n",
    "\n",
    "all_clusters = construct_sets(set_starters,next_nodes)\n",
    "# len(firms)\n",
    "# len(all_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'XRAY': {'XRAY'},\n",
       " 'URI': {'URI'},\n",
       " 'CBG': {'CBG'},\n",
       " 'AIZ': {'AIZ'},\n",
       " 'SYK': {'ABT',\n",
       "  'ADP',\n",
       "  'AFL',\n",
       "  'AIG',\n",
       "  'AJG',\n",
       "  'AME',\n",
       "  'AMG',\n",
       "  'AMP',\n",
       "  'AON',\n",
       "  'BAC',\n",
       "  'BBT',\n",
       "  'BCR',\n",
       "  'BEN',\n",
       "  'BK',\n",
       "  'BLK',\n",
       "  'BRK-B',\n",
       "  'C',\n",
       "  'CB',\n",
       "  'CFG',\n",
       "  'CHD',\n",
       "  'CINF',\n",
       "  'CL',\n",
       "  'CLX',\n",
       "  'CMA',\n",
       "  'COL',\n",
       "  'DHR',\n",
       "  'DPS',\n",
       "  'EMR',\n",
       "  'ETFC',\n",
       "  'ETN',\n",
       "  'FISV',\n",
       "  'FITB',\n",
       "  'GD',\n",
       "  'GS',\n",
       "  'HBAN',\n",
       "  'HIG',\n",
       "  'HON',\n",
       "  'ITW',\n",
       "  'IVZ',\n",
       "  'JNJ',\n",
       "  'JPM',\n",
       "  'KEY',\n",
       "  'KMB',\n",
       "  'KO',\n",
       "  'L',\n",
       "  'LM',\n",
       "  'LMT',\n",
       "  'LNC',\n",
       "  'MA',\n",
       "  'MET',\n",
       "  'MKC',\n",
       "  'MMC',\n",
       "  'MMM',\n",
       "  'MS',\n",
       "  'MTB',\n",
       "  'NOC',\n",
       "  'NTRS',\n",
       "  'PAYX',\n",
       "  'PBCT',\n",
       "  'PCAR',\n",
       "  'PEP',\n",
       "  'PFG',\n",
       "  'PG',\n",
       "  'PGR',\n",
       "  'PH',\n",
       "  'PNC',\n",
       "  'PRU',\n",
       "  'RF',\n",
       "  'ROP',\n",
       "  'SCHW',\n",
       "  'SNA',\n",
       "  'STI',\n",
       "  'STT',\n",
       "  'SWK',\n",
       "  'SYK',\n",
       "  'TMK',\n",
       "  'TROW',\n",
       "  'TRV',\n",
       "  'TXT',\n",
       "  'UNM',\n",
       "  'USB',\n",
       "  'V',\n",
       "  'WFC',\n",
       "  'XL',\n",
       "  'ZION'},\n",
       " 'IR': {'IR'},\n",
       " 'O': {'AIV',\n",
       "  'AVB',\n",
       "  'BXP',\n",
       "  'EQR',\n",
       "  'ESS',\n",
       "  'EXR',\n",
       "  'FRT',\n",
       "  'GGP',\n",
       "  'HCN',\n",
       "  'HCP',\n",
       "  'KIM',\n",
       "  'O',\n",
       "  'PLD',\n",
       "  'PSA',\n",
       "  'SLG',\n",
       "  'SPG',\n",
       "  'UDR',\n",
       "  'VNO',\n",
       "  'VTR'},\n",
       " 'ISRG': {'ISRG'},\n",
       " 'JEC': {'DOV', 'FLR', 'FLS', 'JEC', 'PNR'},\n",
       " 'SLB': {'APA',\n",
       "  'APC',\n",
       "  'BHI',\n",
       "  'COP',\n",
       "  'CVX',\n",
       "  'CXO',\n",
       "  'DO',\n",
       "  'DVN',\n",
       "  'EOG',\n",
       "  'HAL',\n",
       "  'HES',\n",
       "  'HP',\n",
       "  'MRO',\n",
       "  'MUR',\n",
       "  'NBL',\n",
       "  'NFX',\n",
       "  'OXY',\n",
       "  'PXD',\n",
       "  'RIG',\n",
       "  'SLB',\n",
       "  'XEC',\n",
       "  'XOM'},\n",
       " 'SYMC': {'SYMC'},\n",
       " 'HAR': {'HAR'},\n",
       " 'ESRX': {'ESRX'},\n",
       " 'NWS': {'NWS', 'NWSA'},\n",
       " 'NI': {'AEE',\n",
       "  'AEP',\n",
       "  'AWK',\n",
       "  'CMS',\n",
       "  'CNP',\n",
       "  'D',\n",
       "  'DTE',\n",
       "  'DUK',\n",
       "  'ED',\n",
       "  'EIX',\n",
       "  'ES',\n",
       "  'ETR',\n",
       "  'FE',\n",
       "  'LNT',\n",
       "  'NI',\n",
       "  'PCG',\n",
       "  'PEG',\n",
       "  'PNW',\n",
       "  'PPL',\n",
       "  'SCG',\n",
       "  'SO',\n",
       "  'SRE',\n",
       "  'WEC',\n",
       "  'XEL'},\n",
       " 'MRK': {'MRK'},\n",
       " 'WM': {'RSG', 'WM'},\n",
       " 'FLIR': {'FLIR'},\n",
       " 'CMI': {'CMI'},\n",
       " 'AAP': {'AAP'},\n",
       " 'CAG': {'CAG'},\n",
       " 'GRMN': {'GRMN'},\n",
       " 'DLR': {'DLR'},\n",
       " 'QCOM': {'QCOM'},\n",
       " 'EA': {'EA'},\n",
       " 'WYNN': {'WYNN'},\n",
       " 'NRG': {'NRG'},\n",
       " 'VMC': {'MLM', 'VMC'},\n",
       " 'MAC': {'MAC'},\n",
       " 'EQIX': {'EQIX'},\n",
       " 'CTSH': {'CTSH'},\n",
       " 'IP': {'IP'},\n",
       " 'CMG': {'CMG'},\n",
       " 'JNPR': {'JNPR'},\n",
       " 'ABC': {'ABC'},\n",
       " 'AYI': {'AYI'},\n",
       " 'ATVI': {'ATVI'},\n",
       " 'XRX': {'XRX'},\n",
       " 'CTL': {'CTL'},\n",
       " 'AMZN': {'AMZN'},\n",
       " 'DNB': {'DNB'},\n",
       " 'K': {'GIS', 'K'},\n",
       " 'FTI': {'FTI'},\n",
       " 'F': {'F'},\n",
       " 'SNI': {'SNI'},\n",
       " 'MCK': {'MCK'},\n",
       " 'IRM': {'IRM'},\n",
       " 'EMN': {'EMN'},\n",
       " 'FL': {'FL'},\n",
       " 'FMC': {'FMC'},\n",
       " 'ULTA': {'ULTA'},\n",
       " 'DVA': {'DVA'},\n",
       " 'HSY': {'HSY'},\n",
       " 'CA': {'CA'},\n",
       " 'ILMN': {'ILMN'},\n",
       " 'KSS': {'KSS'},\n",
       " 'KSU': {'KSU'},\n",
       " 'STJ': {'STJ'},\n",
       " 'VRSK': {'VRSK'},\n",
       " 'BMY': {'BMY'},\n",
       " 'NEM': {'NEM'},\n",
       " 'JWN': {'JWN'},\n",
       " 'TDG': {'TDG'},\n",
       " 'QRVO': {'QRVO'},\n",
       " 'STZ': {'STZ'},\n",
       " 'JBHT': {'JBHT'},\n",
       " 'NKE': {'NKE'},\n",
       " 'AVY': {'AVY'},\n",
       " 'ADS': {'ADS'},\n",
       " 'ADBE': {'ADBE'},\n",
       " 'SHW': {'ECL', 'PPG', 'SHW'},\n",
       " 'CTAS': {'CTAS'},\n",
       " 'COST': {'COST'},\n",
       " 'DGX': {'DGX', 'LH'},\n",
       " 'CNC': {'AET', 'ANTM', 'CNC', 'UNH'},\n",
       " 'BIIB': {'BIIB'},\n",
       " 'CCI': {'AMT', 'CCI'},\n",
       " 'AGN': {'AGN'},\n",
       " 'KMX': {'KMX'},\n",
       " 'AKAM': {'AKAM'},\n",
       " 'LKQ': {'LKQ'},\n",
       " 'PBI': {'PBI'},\n",
       " 'RAI': {'MO', 'RAI'},\n",
       " 'BSX': {'BSX'},\n",
       " 'HRB': {'HRB'},\n",
       " 'KMI': {'KMI'},\n",
       " 'DFS': {'DFS'},\n",
       " 'BF-B': {'BF-B'},\n",
       " 'XLNX': {'XLNX'},\n",
       " 'TSN': {'TSN'},\n",
       " 'MU': {'MU'},\n",
       " 'RTN': {'RTN'},\n",
       " 'GPN': {'GPN'},\n",
       " 'BBY': {'BBY'},\n",
       " 'NFLX': {'NFLX'},\n",
       " 'LLL': {'LLL'},\n",
       " 'COH': {'COH'},\n",
       " 'ALLE': {'ALLE'},\n",
       " 'SE': {'OKE', 'SE'},\n",
       " 'RHT': {'RHT'},\n",
       " 'SIG': {'SIG'},\n",
       " 'FSLR': {'FSLR'},\n",
       " 'GE': {'GE'},\n",
       " 'EXPD': {'CHRW', 'EXPD'},\n",
       " 'INTC': {'INTC'},\n",
       " 'DE': {'DE'},\n",
       " 'KR': {'KR'},\n",
       " 'EXC': {'EXC'},\n",
       " 'VAR': {'VAR'},\n",
       " 'HBI': {'HBI'},\n",
       " 'PM': {'PM'},\n",
       " 'VIAB': {'VIAB'},\n",
       " 'GPS': {'GPS'},\n",
       " 'PCLN': {'PCLN'},\n",
       " 'CMCSA': {'CMCSA'},\n",
       " 'VFC': {'VFC'},\n",
       " 'HAS': {'HAS'},\n",
       " 'TAP': {'TAP'},\n",
       " 'PFE': {'PFE'},\n",
       " 'MSI': {'MSI'},\n",
       " 'NOV': {'NOV'},\n",
       " 'EMC': {'EMC'},\n",
       " 'LLY': {'LLY'},\n",
       " 'ACN': {'ACN'},\n",
       " 'WAT': {'WAT'},\n",
       " 'HOLX': {'HOLX'},\n",
       " 'R': {'R'},\n",
       " 'RCL': {'CCL', 'RCL'},\n",
       " 'INTU': {'INTU'},\n",
       " 'FAST': {'FAST'},\n",
       " 'SPLS': {'SPLS'},\n",
       " 'PSX': {'MPC', 'PSX', 'TSO', 'VLO'},\n",
       " 'YHOO': {'YHOO'},\n",
       " 'UTX': {'UTX'},\n",
       " 'HOT': {'HOT', 'MAR', 'WYN'},\n",
       " 'MDLZ': {'MDLZ'},\n",
       " 'CTXS': {'CTXS'},\n",
       " 'NDAQ': {'CME', 'ICE', 'NDAQ'},\n",
       " 'IBM': {'IBM'},\n",
       " 'MAT': {'MAT'},\n",
       " 'COG': {'COG'},\n",
       " 'FTR': {'FTR'},\n",
       " 'SYY': {'SYY'},\n",
       " 'HRS': {'HRS'},\n",
       " 'CERN': {'CERN'},\n",
       " 'GWW': {'GWW'},\n",
       " 'LYB': {'LYB'},\n",
       " 'NUE': {'NUE'},\n",
       " 'HPQ': {'HPQ'},\n",
       " 'MON': {'MON'},\n",
       " 'BLL': {'BLL'},\n",
       " 'XYL': {'XYL'},\n",
       " 'FOX': {'FOX', 'FOXA'},\n",
       " 'AAPL': {'AAPL'},\n",
       " 'ALXN': {'ALXN'},\n",
       " 'SBUX': {'SBUX'},\n",
       " 'LEG': {'LEG'},\n",
       " 'EL': {'EL'},\n",
       " 'DOW': {'DOW'},\n",
       " 'NVDA': {'NVDA'},\n",
       " 'VRTX': {'VRTX'},\n",
       " 'RL': {'RL'},\n",
       " 'EXPE': {'EXPE'},\n",
       " 'WFM': {'WFM'},\n",
       " 'UA': {'UA'},\n",
       " 'DLPH': {'BWA', 'DLPH'},\n",
       " 'APD': {'APD'},\n",
       " 'GOOG': {'GOOG', 'GOOGL'},\n",
       " 'SEE': {'SEE'},\n",
       " 'ALL': {'ALL'},\n",
       " 'WY': {'WY'},\n",
       " 'WMT': {'WMT'},\n",
       " 'DIS': {'DIS'},\n",
       " 'FBHS': {'FBHS'},\n",
       " 'FB': {'FB'},\n",
       " 'SPGI': {'MCO', 'SPGI'},\n",
       " 'NTAP': {'NTAP'},\n",
       " 'AMAT': {'AMAT'},\n",
       " 'MDT': {'HSIC', 'MDT'},\n",
       " 'HRL': {'HRL'},\n",
       " 'VZ': {'T', 'VZ'},\n",
       " 'YUM': {'YUM'},\n",
       " 'ADSK': {'ADSK'},\n",
       " 'CBS': {'CBS'},\n",
       " 'ZBH': {'ZBH'},\n",
       " 'DISCK': {'DISCA', 'DISCK'},\n",
       " 'MCD': {'MCD'},\n",
       " 'NAVI': {'NAVI'},\n",
       " 'HOG': {'HOG'},\n",
       " 'NLSN': {'NLSN'},\n",
       " 'ALB': {'ALB'},\n",
       " 'MYL': {'MYL'},\n",
       " 'CRM': {'CRM'},\n",
       " 'MHK': {'MHK'},\n",
       " 'RHI': {'RHI'},\n",
       " 'TSS': {'TSS'},\n",
       " 'MCHP': {'ADI', 'LLTC', 'MCHP', 'TXN'},\n",
       " 'MNST': {'MNST'},\n",
       " 'MSFT': {'MSFT'},\n",
       " 'GM': {'GM'},\n",
       " 'LUV': {'AAL', 'ALK', 'DAL', 'LUV', 'UAL'},\n",
       " 'KORS': {'KORS'},\n",
       " 'CAH': {'CAH'},\n",
       " 'EBAY': {'EBAY'},\n",
       " 'FCX': {'FCX'},\n",
       " 'VRSN': {'VRSN'},\n",
       " 'AN': {'AN'},\n",
       " 'ORLY': {'ORLY'},\n",
       " 'BDX': {'BDX'},\n",
       " 'TYC': {'JCI', 'TYC'},\n",
       " 'WBA': {'WBA'},\n",
       " 'SJM': {'SJM'},\n",
       " 'AVGO': {'AVGO'},\n",
       " 'ADM': {'ADM'},\n",
       " 'MAS': {'MAS'},\n",
       " 'EFX': {'EFX'},\n",
       " 'SWN': {'RRC', 'SWN'},\n",
       " 'NWL': {'NWL'},\n",
       " 'EW': {'EW'},\n",
       " 'LUK': {'LUK'},\n",
       " 'WU': {'WU'},\n",
       " 'BBBY': {'BBBY'},\n",
       " 'GILD': {'GILD'},\n",
       " 'PVH': {'PVH'},\n",
       " 'GPC': {'GPC'},\n",
       " 'TRIP': {'TRIP'},\n",
       " 'M': {'M'},\n",
       " 'SYF': {'SYF'},\n",
       " 'SWKS': {'SWKS'},\n",
       " 'LB': {'LB'},\n",
       " 'LRCX': {'LRCX'},\n",
       " 'AXP': {'AXP'},\n",
       " 'MNK': {'MNK'},\n",
       " 'STX': {'STX'},\n",
       " 'ZTS': {'ZTS'},\n",
       " 'TEL': {'TEL'},\n",
       " 'TGNA': {'TGNA'},\n",
       " 'TGT': {'TGT'},\n",
       " 'LVLT': {'LVLT'},\n",
       " 'TDC': {'TDC'},\n",
       " 'PRGO': {'PRGO'},\n",
       " 'AA': {'AA'},\n",
       " 'DG': {'DG'},\n",
       " 'URBN': {'URBN'},\n",
       " 'PWR': {'PWR'},\n",
       " 'PDCO': {'PDCO'},\n",
       " 'CI': {'CI'},\n",
       " 'BAX': {'BAX'},\n",
       " 'PHM': {'DHI', 'LEN', 'PHM'},\n",
       " 'FFIV': {'FFIV'},\n",
       " 'DLTR': {'DLTR'},\n",
       " 'CELG': {'AMGN', 'CELG', 'REGN'},\n",
       " 'CPB': {'CPB'},\n",
       " 'UPS': {'FDX', 'UPS'},\n",
       " 'HST': {'HST'},\n",
       " 'AES': {'AES'},\n",
       " 'COF': {'COF'},\n",
       " 'CAT': {'CAT'},\n",
       " 'TWX': {'TWX'},\n",
       " 'OI': {'OI'},\n",
       " 'ROK': {'ROK'},\n",
       " 'TMO': {'A', 'PKI', 'TMO'},\n",
       " 'ORCL': {'ORCL'},\n",
       " 'EQT': {'EQT'},\n",
       " 'LOW': {'HD', 'LOW'},\n",
       " 'ABBV': {'ABBV'},\n",
       " 'MOS': {'MOS'},\n",
       " 'DRI': {'DRI'},\n",
       " 'GT': {'GT'},\n",
       " 'TIF': {'TIF'},\n",
       " 'TSCO': {'TSCO'},\n",
       " 'TJX': {'ROST', 'TJX'},\n",
       " 'ENDP': {'ENDP'},\n",
       " 'NSC': {'CSX', 'NSC'},\n",
       " 'GLW': {'GLW'},\n",
       " 'BA': {'BA'},\n",
       " 'WHR': {'WHR'},\n",
       " 'CF': {'CF'},\n",
       " 'APH': {'APH'},\n",
       " 'MJN': {'MJN'},\n",
       " 'IFF': {'IFF'},\n",
       " 'WMB': {'WMB'},\n",
       " 'PX': {'PX'},\n",
       " 'OMC': {'IPG', 'OMC'},\n",
       " 'HUM': {'HUM'},\n",
       " 'DD': {'DD'},\n",
       " 'CHK': {'CHK'},\n",
       " 'AZO': {'AZO'},\n",
       " 'WDC': {'WDC'},\n",
       " 'UNP': {'UNP'},\n",
       " 'SRCL': {'SRCL'},\n",
       " 'UHS': {'HCA', 'UHS'},\n",
       " 'CSCO': {'CSCO'},\n",
       " 'CVS': {'CVS'},\n",
       " 'FIS': {'FIS'},\n",
       " 'KLAC': {'KLAC'}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2: analysing the results\n",
    "\n",
    "After you have implemented the algorithm in Python, add cells below answering the following questions:\n",
    "- Do some detective work: what is the algorithm that you've implemented called? In what other graph problem is it often used? How are the problems related? (Hint: the algorithm is mentioned on the Wikipedia page for greedy algorithms.)\n",
    "- Run the algorithm and present the results formatted in a useful way. \n",
    "- Discuss the results for different values of $k$.  \n",
    "- Do the resulting clusters \"make sense\"? (You may need to search online what the companies do.) Verify that the stocks in your clusters perform similarly by plotting the returns and the (normalised) stock prices for some of the clusters.\n",
    "- You may use graphs etc. to present your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kruskal's algorithm, it is often used when finding a minimum spanning tree.\n",
    "- bigger k, less number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: \n",
    "\n",
    "**40% of grade**\n",
    "\n",
    "Depending on your interests, you may work on either subsection below, or both. You might go deeper into one question than another, but for an outstanding grade, you should have at least some discussion on both.\n",
    "\n",
    "You may use generative AI such as chatGPT to help with this part of the assignment (but not the other parts).\n",
    "\n",
    "### In-depth analysis\n",
    "\n",
    "The project is _open_ in the sense that you can probably think of further interesting questions to look into based on returns, correlations, and clusters. This is not required but being creative and going further than the above questions will make your work stand out. You can explore one or several of the ideas below, or come up with questions of your own.\n",
    "\n",
    "Depending on your interests, you might look at different things. For example, when researching the algorithm, you might be interested in its complexity, and how to improve your implementation's efficiency. On Wikipedia, you may find a couple of ways to drastically improve the algorithm speed, but are relatively small changes to your code.\n",
    "\n",
    "If you're more interested in the financial applications of clustering, there are also opportunities to think about further steps. For example, some people claim that you can derive trading strategies based on clustering - that often one of the stocks in a cluster is a leader and the others follow that price. If this is true, you could track the price of the leader stock and then trade the other stocks in the cluster based on changes in the leader's price. Do you think this would make sense? Do you have an idea on how to identify a leader stock?\n",
    "\n",
    "You might also want to repeat the analysis for different time periods. You would be able to do this by looking at the code for the second homework to figure out how to read data from Yahoo Finance using pandas, and going through the process for all companies in the csv file for another time period. Perhaps you could explore for example how correlations between companies have changed over time, or how clusters found by your algorithm change over time.\n",
    "\n",
    "### Exploring other clustering methods\n",
    "\n",
    "You've used just one approach to clustering, and arguably not the best one. Research clustering algorithms and libraries to apply them in Python. Discuss some other algorithms that could be used, and how they differ from the one you've implemented. Look at the Python library `scikit-learn`. How would you apply the clustering algorithms provided by the library to stock price data? Would you need to develop new metrics other than correlations? If you want to go even further,  try running some of these other clustering algorithms on your data, and report the results. Start from here: http://scikit-learn.org/stable/modules/clustering.html#clustering; you'll find a stock market example there too. For future reference, you may also find other interesting machine-learning tools for both stock market analysis or other analytics purposes.\n",
    "\n",
    "### Question 4\n",
    "\n",
    "Create cells below to add your extra part as code and narrative text explaining your idea and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All done!\n",
    "\n",
    "Create a zip file containing your submission and upload it on the Hub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e72bfbb931b519542a84a43921682ca2a4773977d3268be7c129df57f2af88e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
